{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69a0785c",
   "metadata": {},
   "source": [
    "# Script to fit the model to experiments of Aggregation/Breakup of cyanobacterial colonies\n",
    "### Experiments: Size distribution of Microcystis V163 colonies in turbulent cone-and-plate shear \n",
    "### Model: Classes method for size distribution in homogeneous turbulence\n",
    "### Yuri Sinzato - PhD Student - University of Amsterdam\n",
    "### Last version: 26-06-2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e125ecc",
   "metadata": {},
   "source": [
    "#### Brief description of the routine:\n",
    "- This routine is used to fit the numerical model of aggregation/breakup of aggregates to the experimental results of cyanobacterial colonies in shear\n",
    "- The experimental results display the time-evolution of the size distribution (and its moments) of Microcystis V163 colonies in tubulent cone-and-plate shear for various concentrations and turbulence intensities\n",
    "- The numerical model simulates the evolution of the size distribution of two populations (division formed colonies and aggregated colonies) subjected to aggregation and breakage \n",
    "- The parameters of the numerical model are fitted to the experiments with a least square minimization of the main descriptors of the size distribution (medians and main percentiles). \n",
    "- The numerical model is run with the executable 'Numerical_Model/agg_bre.exe.'\n",
    "- The initial size distribution is extracted from the experimental data, and supplied to the model via the text file 'Numerical_Model/Temporary/csd_initial.txt'\n",
    "- The parameters are supplied to the model via the text file 'Numerical_Model/Temporary/parameters.txt'\n",
    "- The results of the simulation are returned in the file via the text file 'Numerical_Model/Temporary/r_data.txt'\n",
    "- The numerical results must be interpolated in the time to allow direct comparison with the experimental data\n",
    "- The minimization is made with the library scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3cb580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pims\n",
    "import os\n",
    "import scipy\n",
    "from IPython.display import clear_output\n",
    "#import time\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4832b7f1",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20e77b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Function to smoothen a noisy initial size distribution and return the interpolated value \n",
    "def smooth_cmd(cmd_exp,r_bin_num,r_bin_exp):\n",
    "    cmd_exp_smooth = np.zeros(N_r_exp)\n",
    "    cmd_num = np.zeros(N_r)\n",
    "    for i in range(N_r_exp):     \n",
    "        #for j in range(max(math.floor(i*h_r/5.0),1)): # Smooth\n",
    "        for j in range(1):  # Not smooth\n",
    "            cmd_exp_smooth[i] = sum(cmd_exp[i-j:min(i+j+1,N_r_exp)])/(1+2*j)\n",
    "    cmd_num = np.interp(r_bin_num,r_bin_exp,cmd_exp_smooth)\n",
    "    return cmd_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88b67ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to import experimental data file, run simulation and calculate penalty function\n",
    "def penalty_function(folder_path,phi_o,epsilon,fit_par):\n",
    "    #  fit_par is an array with the following order parameter = [alpha,ep_s1,ep_s2,q]\n",
    "    ### Import initial experimental csd data file\n",
    "    exp_file_path = folder_path+'csd_data.txt'\n",
    "    r_bin_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=2,usecols=(0),max_rows = N_r_exp) # Read experimental r_bin centers\n",
    "    cmd_s_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=2,usecols=(2),max_rows = N_r_exp) # Read experimental cmd for small colonies\n",
    "    cmd_l_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=2,usecols=(3),max_rows = N_r_exp)  # Read experimental cmd for large colonies\n",
    "    \n",
    "    ### Import time evolution of statistical descriptors of experimental data\n",
    "    exp_file_path = folder_path+'r_data.txt'\n",
    "    time_array_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(0),max_rows = N_t_fit)  # Create array for time points\n",
    "    frac_small_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(7),max_rows = N_t_fit)  # Create array for fraction of small colonies (kind1)\n",
    "    r_med1_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(8),max_rows = N_t_fit)  # Create array for median of small colonies (kind1)\n",
    "    lr_med1_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(9),max_rows = N_t_fit)  # Create array for percentile 25 of small colonies (kind1)\n",
    "    hr_med1_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(10),max_rows = N_t_fit)  # Create array for percentile 75 of small colonies(kind1)\n",
    "    r_med2_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(11),max_rows = N_t_fit)  # Create array for median of large colonies (kind2)\n",
    "    lr_med2_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(12),max_rows = N_t_fit)  # Create array for percentile 25 of large colonies (kind2)\n",
    "    hr_med2_exp = np.genfromtxt(exp_file_path,dtype='float',skip_header=1,usecols=(13),max_rows = N_t_fit)  # Create array for percentile 75 of large colonies (kind2)\n",
    "\n",
    "\n",
    "    alpha = fit_par[0] # Stickiness\n",
    "    ep_s1 = fit_par[1] # Critical dissipation energy small colonies\n",
    "    ep_s2 = fit_par[2] # Critical dissipation energy large colonies\n",
    "    q1 = fit_par[3] # Exponent of critical size for small colonies\n",
    "    q2 = fit_par[4] # Exponent of critical size for large colonies\n",
    "    #alpha = alpha_i # Stickiness\n",
    "    #ep_s1 = ep_s1_i # Critical dissipation energy small colonies\n",
    "    #ep_s2 = ep_s2_i #2.02e4 # Critical dissipation energy large colonies\n",
    "    #q = q_i # Exponent of critical size\n",
    "    \n",
    "    ### Write parameters file\n",
    "    para_file = open('Numerical_Model/Temporary/parameters.txt', 'w')  # Size distributions \n",
    "    para_file.write('N_a \\t {:d} \\n'.format(N_a) )\n",
    "    para_file.write('N_r \\t {:d} \\n'.format(N_r) )\n",
    "    para_file.write('h_r \\t {:12.6f} \\n'.format(h_r) )\n",
    "    para_file.write('t_exp \\t {:12.6f} \\n'.format(t_exp) )\n",
    "    para_file.write('N_tmax \\t {:d} \\n'.format(N_tmax) )\n",
    "    para_file.write('Dt_i \\t {:12.6f} \\n'.format(Dt_i) )\n",
    "    para_file.write('Dt_l \\t {:12.6f} \\n'.format(Dt_l) )\n",
    "    para_file.write('Dt_u \\t {:12.6f} \\n'.format(Dt_u) )\n",
    "    para_file.write('tol_csd \\t {:.5e} \\n'.format(tol_csd) )\n",
    "    para_file.write('mu \\t {:12.6f} \\n'.format(mu) )\n",
    "    para_file.write('alpha \\t {:.5e} \\n'.format(alpha) )\n",
    "    para_file.write('phi_o \\t {:.5e} \\n'.format(phi_o) )\n",
    "    para_file.write('ep_s1 \\t {:.5e} \\n'.format(ep_s1) )\n",
    "    para_file.write('ep_s2 \\t {:.5e} \\n'.format(ep_s2) )\n",
    "    para_file.write('d_f \\t {:12.6f} \\n'.format(d_f) )\n",
    "    para_file.write('q \\t {:12.6f} \\n'.format(q1) )\n",
    "    para_file.write('q \\t {:12.6f} \\n'.format(q2) )\n",
    "    para_file.write('epsilon \\t {:.5e} \\n'.format(epsilon) )\n",
    "    para_file.write('nu \\t {:.5e} \\n'.format(nu) )\n",
    "    para_file.close()\n",
    "\n",
    "    ### Write initial csd data file for simulation\n",
    "    #Generate initial num csd by smoothing exp csd\n",
    "    r_bin_num = np.linspace(0.5*h_r,(N_r+0.5)*h_r,N_r,endpoint = False)\n",
    "    cmd_s_num = smooth_cmd(cmd_s_exp,r_bin_num,r_bin_exp) # Smoothing function in csd of small colonies\n",
    "    if sum(cmd_s_num) > 0:\n",
    "        cmd_s_num = cmd_s_num*sum(cmd_s_exp)/sum(cmd_s_num) # Correct normalization\n",
    "    cmd_l_num = smooth_cmd(cmd_l_exp,r_bin_num,r_bin_exp) # Smoothing function in csd of large colonies\n",
    "    if sum(cmd_l_num) > 0:\n",
    "        cmd_l_num = cmd_l_num*sum(cmd_l_exp)/sum(cmd_l_num) # Correct normalization\n",
    "    # Write in text file\n",
    "    csd_initial_file = open('Numerical_Model/Temporary/csd_initial.txt', 'w')  # Size distributions \n",
    "    csd_initial_file.write('#Initial Normalized size distribution\\n')\n",
    "    csd_initial_file.write('#(1)Block numbe            (2)t(s)                (3)r      (4)csd_f(norm)     (5)csd_m(kind1-norm (6)csd_m(kind2-norm\\n')\n",
    "    for i in range(N_r):\n",
    "        csd_initial_file.write('{:15d}'.format(0))\n",
    "        csd_initial_file.write('{:20.6f}'.format(0))\n",
    "        csd_initial_file.write('{:20.10f}'.format(r_bin_num[i]))\n",
    "        csd_initial_file.write('{:20.10f}'.format(cmd_s_num[i]))\n",
    "        csd_initial_file.write('{:20.10f}'.format(cmd_s_num[i]))\n",
    "        csd_initial_file.write('{:20.10f}\\n'.format(cmd_l_num[i]))\n",
    "    csd_initial_file.close()\n",
    "\n",
    "    ### Run simulation\n",
    "    os.system(\"wsl ./Numerical_Model/agg_bre.exe\")\n",
    "\n",
    "    ### Import time evolution of statistical descriptors of numerical data - Data is interpolated to fit format of experimental data\n",
    "    file_name = 'Numerical_Model/Temporary/r_data.txt'  # File path\n",
    "    time_array_num = np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(0))  # Create array for time points\n",
    "    frac_small_num = np.interp(time_array_exp,time_array_num,np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(7)))  # Create array for fraction of small colonies (kind1)\n",
    "    r_med1_num = np.interp(time_array_exp,time_array_num,np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(8)))  # Create array for median of small colonies (kind1)\n",
    "    lr_med1_num = np.interp(time_array_exp,time_array_num,np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(9)))  # Create array for percentile 25 of small colonies (kind1)\n",
    "    hr_med1_num = np.interp(time_array_exp,time_array_num,np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(10)))  # Create array for percentile 75 of small colonies(kind1)\n",
    "    r_med2_num = np.interp(time_array_exp,time_array_num,np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(11)))  # Create array for median of large colonies (kind2)\n",
    "    lr_med2_num = np.interp(time_array_exp,time_array_num,np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(12)))  # Create array for percentile 25 of large colonies (kind2)\n",
    "    hr_med2_num = np.interp(time_array_exp,time_array_num,np.genfromtxt(file_name,dtype='float',skip_header=1,usecols=(13)))  # Create array for percentile 75 of large colonies (kind2)\n",
    "\n",
    "    ### Calculate penalty function - squared difference btw num and exp statistical descriptors\n",
    "    penalty = 0\n",
    "    penalty += np.sum(np.abs(frac_small_num - frac_small_exp))\n",
    "    penalty += np.sum(frac_small_exp*np.abs(r_med1_num - r_med1_exp)/np.abs(r_med1_num))\n",
    "    penalty += np.sum(frac_small_exp*np.abs(lr_med1_num - lr_med1_exp)/np.abs(r_med1_num))\n",
    "    penalty += np.sum(frac_small_exp*np.abs(hr_med1_num - hr_med1_exp)/np.abs(r_med1_num))\n",
    "    penalty += np.sum((np.ones(len(frac_small_exp))-frac_small_exp)*np.abs(r_med2_num - r_med2_exp)/np.abs(r_med2_num))\n",
    "    penalty += np.sum((np.ones(len(frac_small_exp))-frac_small_exp)*np.abs(lr_med2_num - lr_med2_exp)/np.abs(r_med2_num))\n",
    "    penalty += np.sum((np.ones(len(frac_small_exp))-frac_small_exp)*np.abs(hr_med2_num - hr_med2_exp)/np.abs(r_med2_num))\n",
    "\n",
    "    print('Folder: '+str(folder_path))\n",
    "    print('penalty_function:'+str(penalty))\n",
    "    \n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8114e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to iterate penalty function through various data files\n",
    "\n",
    "def total_penalty(fit_par): # Selected data file\n",
    "    total_penalty_value = 0\n",
    "    for i in range(3,4):\n",
    "        total_penalty_value += penalty_function(folder_paths[i],phi_o_values[i],epsilon_values[i],fit_par)\n",
    "    print('Total penalty function: '+str(total_penalty_value))\n",
    "    print('Fitting parameters: '+str(fit_par))\n",
    "    return total_penalty_value\n",
    "\n",
    "def total_penalty_bre(fit_par): # Only breakage data files\n",
    "    total_penalty_value = 0\n",
    "    fit_par_comb = np.concatenate((fit_par_i[0:2],fit_par[0:1],fit_par_i[3:4],fit_par[1:2]))\n",
    "    for i in [0,1,3,4]:\n",
    "        total_penalty_value += penalty_function(folder_paths[i],phi_o_values[i],epsilon_values[i],fit_par_comb)\n",
    "    \n",
    "    print('Total penalty function: '+str(total_penalty_value))\n",
    "    print('Fitting parameters: '+str(fit_par_comb))\n",
    "    return total_penalty_value\n",
    "\n",
    "def total_penalty_agg(fit_par): # Only aggregation data files\n",
    "    total_penalty_value = 0\n",
    "    fit_par_comb = np.concatenate((fit_par[0:2],fit_par_i[2:3],fit_par[2:3],fit_par_i[4:5]))\n",
    "    for i in range(5,9):\n",
    "        total_penalty_value += penalty_function(folder_paths[i],phi_o_values[i],epsilon_values[i],fit_par_comb)\n",
    "    print('Total penalty function: '+str(total_penalty_value))\n",
    "    print('Fitting parameters: '+str(fit_par_comb))\n",
    "    return total_penalty_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "831ba5e5",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6103b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical parameters\n",
    "mu = 0.0e-4 # Growth rate\n",
    "d_f = 2.09 # 3-D fractal dimension\n",
    "nu = 1.002e-6 # Kinematic viscosity\n",
    "\n",
    "# Binning parameters - Exp\n",
    "N_t_fit = 25 ## Time points to use in fit\n",
    "h_r_exp = 0.5 ## Bin size for experimental data\n",
    "N_r_exp = 400 ## Number of bins for experimental data\n",
    "\n",
    "# Binning parameters - Num\n",
    "R_a = 10.0 # Range aggregative bins\n",
    "R_r = 80.0 # Range of total bins - Should be at least twice the aggregative range\n",
    "h_r = 0.5 # Bin size for numerical model\n",
    "N_a = (np.ceil(R_a/h_r)).astype(int) # Number of aggregative bins\n",
    "N_r = (np.ceil(R_r/h_r)).astype(int)  # Total number of bins\n",
    "\n",
    "# Numerical routine parameters \n",
    "t_exp = 20000 # Simulation time\n",
    "N_tmax = 10000000 # Maximum number of time steps in simulation\n",
    "Dt_i = 1000 # Initial time step (non-dimensional)\n",
    "Dt_l = 1 # Minimum time step (non-dimensional)\n",
    "Dt_u = 1e5 # Maximum time step (non-dimensional)\n",
    "tol_csd = 5e-4 # Tolerance for mass distribution\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d39ca3f",
   "metadata": {},
   "source": [
    "### Experimental data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "361d79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = np.array(['Bre_100s_c1e6_23-11-22/Data/',\n",
    "                         'Bre_500s_c1e6_15-3-2/Data/',\n",
    "                         'Bre_800s_c1e6_11-4-23/Data/',\n",
    "                         'Bre_1000s_c1e6_11-1-23/Data/',\n",
    "                         'Bre_1200s_c1e6_1-5-23/Data/',\n",
    "                         'Agg_100s_c1e6_24-11-22/Data/',\n",
    "                         'Agg_100s_c2e6_2-12-22/Data/',\n",
    "                         'Agg_100s_c5e6_6-1-23/Data/',\n",
    "                         'Agg_1000s_c1e6_12-1-23/Data/',\n",
    "                         'Bre_100s_c2e6_8-12-22/Data/',\n",
    "                         'Bre_100s_c5e6_1-12-22/Data/'\n",
    "                         ])  # File path\n",
    "d_eq1 = 5.5 # Single cell equivalent circular diameter in um (Calculated from projected area)\n",
    "v1 = math.pi*d_eq1**3/6 # Single cell equivalent volume in um3 (Calculated from equivalent circular diameter)\n",
    "phi_o_values = np.array([1.0e6*v1*1e-12,\n",
    "                         1.0e6*v1*1e-12,\n",
    "                         1.0e6*v1*1e-12,\n",
    "                         1.0e6*v1*1e-12,\n",
    "                         1.0e6*v1*1e-12,\n",
    "                         1.0e6*v1*1e-12,\n",
    "                         2.0e6*v1*1e-12,\n",
    "                         5.0e6*v1*1e-12,\n",
    "                         1.0e6*v1*1e-12,\n",
    "                         2.0e6*v1*1e-12,\n",
    "                         5.0e6*v1*1e-12]) # Volume fraction of cells\n",
    "epsilon_values = np.array([0.0192,\n",
    "                          1.079,\n",
    "                          3.493,\n",
    "                          5.784,\n",
    "                          9.627,\n",
    "                          0.0192,\n",
    "                          0.0192,\n",
    "                          0.0192,\n",
    "                          5.784,\n",
    "                          0.0192,\n",
    "                          0.0192]) # Energy dissipation rate (m2/s3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5399b5d",
   "metadata": {},
   "source": [
    "## Minimization routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfac39ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: Bre_1200s_c1e6_1-5-23/Data/\n",
      "penalty_function:81.560618741144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81.560618741144"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial guess of fitting parameters\n",
    "alpha_i =  2.265e-02 #2.44200000e-02   # Stickiness\n",
    "ep_s1_i =  3.412e-02 # 4.28100000e-02 # Critical dissipation energy small colonies\n",
    "ep_s2_i =  3.142e+01 # 5.43597852e+01  # Critical dissipation energy large colonies\n",
    "q1_i = 4.525e+00 # 2.69700000e+00  # Exponent of critical size for small colonies\n",
    "q2_i =  4.138e+00 # 3.04903564e+00  # Exponent of critical size for large colonies\n",
    "fit_par_i= np.array([alpha_i,ep_s1_i,ep_s2_i,q1_i,q2_i])\n",
    "#fit_par_i= np.array([alpha_i,ep_s1_i])\n",
    "#fit_par_i= np.array([ep_s2_i,q_i])\n",
    "\n",
    "\n",
    "\n",
    "#Minimize penalty function to fit parameters\n",
    "file_num = 4\n",
    "penalty_function(folder_paths[file_num],phi_o_values[file_num],epsilon_values[file_num],fit_par_i)\n",
    "#total_penalty(fit_par_i)\n",
    "\n",
    "# Fit using all data files\n",
    "#fit_result = scipy.optimize.minimize(total_penalty, fit_par_i,method='Nelder-Mead', bounds=[(0,0.2),(0.01,30),(10,200),(1.5,6),(1.5,6)], options={'maxiter': 100, 'disp': True})\n",
    "#fit_file = open('fit_results.txt', 'w')  # Size distributions \n",
    "# Fit using only breakage files\n",
    "#fit_result = scipy.optimize.minimize(total_penalty_bre, np.concatenate((fit_par_i[2:3],fit_par_i[4:5])),method='Nelder-Mead', bounds=[(10,200),(2,7)], options={'maxiter': 30, 'disp': True})\n",
    "#fit_file = open('fit_results_breakage.txt', 'w')  # Size distributions \n",
    "# Fit using only aggregation files\n",
    "#fit_result = scipy.optimize.minimize(total_penalty_agg, np.concatenate((fit_par_i[0:2],fit_par_i[3:4])),method='Nelder-Mead', bounds=[(0,0.2),(0.02,0.1),(2.0,6.0)], options={'maxiter': 50, 'disp': True})\n",
    "#fit_file = open('fit_results_aggregation.txt', 'w')  # Size distributions \n",
    "# Write the fit result\n",
    "#fit_file.write(str(fit_result))\n",
    "#fit_file.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
